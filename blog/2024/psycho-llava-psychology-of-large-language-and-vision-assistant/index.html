<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p>Here we are curious to see how a vision language does when it comes to Stroop Color and Word Test. In humans, there is a very interesting effect called the Stroop effect (seeÂ <a href="https://en.wikipedia.org/wiki/Stroop_effect" rel="external nofollow noopener" target="_blank">here</a>).</p> <p>Please go <a href="https://faculty.washington.edu/chudler/java/ready.html" rel="external nofollow noopener" target="_blank">here</a> and do the experiment first and then come back, itâ€™sÂ fun!!</p> <p>Now let's see how does LLaVA the task. It will blow yourÂ mind!</p> <blockquote>ğŸŒ‹ <a href="https://llava-vl.github.io/" rel="external nofollow noopener" target="_blank">LLaVA: Large Language and Vision Assistant</a> </blockquote> <h3>LLaVA (llava-v1.5â€“13b-4bit)</h3> <p>First, we ar using <a href="https://huggingface.co/spaces/badayvedat/LLaVA" rel="external nofollow noopener" target="_blank">this</a> demo on HF.<br>Letâ€™s try a few examples:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/640/1*qfIqbvuSPe7S81zMPfrkdA.png"><figcaption>What do you see? Did you say â€œBlueâ€ orÂ Redâ€?</figcaption></figure> <p>We give the model a picture of the word â€œBLUEâ€ with red ink and ask the model â€œIn this experiment, you are required to say the color of the word, not what the word says. one wordâ€. What would youÂ expect?</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*4K74RJKB5iek4HZcFCWj7w.png"><figcaption>Wrong!!!</figcaption></figure> <p>It was pretty disappointing! But what about the newerÂ version?</p> <h3>LLaVA (llava-v1.6â€“34b)</h3> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*67LjdHDnlfDyIX81ZdrxKw.png"><figcaption>Wow!!</figcaption></figure> <p>It did really well, and it seems that the newÂ version</p> <blockquote>â€œ<a href="https://llava-vl.github.io/blog/2024-01-30-llava-next/" rel="external nofollow noopener" target="_blank">LLaVA-NeXT: Improved reasoning, OCR, and world knowledge</a>â€</blockquote> <p>has improved!</p> <p>But can we still fool it? letâ€™sÂ try!</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*N1LlpkyjPqSCifghnPvmLg.png"><figcaption>Wow! it is impressive!</figcaption></figure> <p>It seems the newest version of LLaVA really has an understanding ofÂ images!</p> <p>So to answer our question, does LLaVA have the StroopÂ effect?</p> <p>It seems that the LLaVA v1.6 is pretty robust and does not show any sign of theÂ effect!</p> <p>But wait! Do you think the model can do the taskÂ right?</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*-H8Etn6NkuaFN49v_9B1Gw.png"><figcaption>The model focused on text!!! And wrote the words in the first row correctly but failed theÂ rest!</figcaption></figure> <p>So maybe the model has the StroopÂ effect?</p> <p>Another failureÂ example:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*HI7lo7pD4V5Ky8R-9t_omw.png"><figcaption>Another failure example ofÂ LLaVA</figcaption></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*RzaWSk5SdhI-8p_kB8fMOg.png"><figcaption>Another failure example ofÂ LLaVA</figcaption></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*UD6M4gFbCjwkr76-SixZZg.png"><figcaption>Another failure example of LLaVA, the model keeps generating!!!</figcaption></figure> <p>What if we create a dataset and try to evaluate the model? A dataset of images with 5by5 grids of colourful words. We benchmarked `llava-hf/llava-v1.6-mistral-7b-hfâ€™ models with 200Â samples.</p> <p>for this prompt â€œ In this experiment you are required to say the color of the word, not what the word says. Read the list as fast as you can.â€ the scoresÂ are:</p> <pre>{'rouge1': 0.2468631578947369, 'rouge2': 0.05287940379403794, 'rougeL': 0.171221052631579, 'rougeLsum': 0.17204210526315794}<br>{'bleu': 0.08737648301423374, 'precisions': [0.19452054794520549, 0.13663911845730028, 0.08116343490304709, 0.02701949860724234], 'brevity_penalty': 1.0, 'length_ratio': 3.7244897959183674, 'translation_length': 3650, 'reference_length': 980}</pre> <p>for this prompt â€œ In this experiment you are required to say what the word says, not what the color of the word is. Read the list as fast as you can.â€ the scoresÂ are:</p> <pre>{'rouge1': 0.40956600104156504, 'rouge2': 0.18901720719876702, 'rougeL': 0.3243491992969437, 'rougeLsum': 0.32076109917651274}<br>{'bleu': 0.15035696220994574, 'precisions': [0.26395039858281666, 0.18453976764968721, 0.130297565374211, 0.0805277525022748], 'brevity_penalty': 1.0, 'length_ratio': 2.304081632653061, 'translation_length': 2258, 'reference_length': 980}So why does the model do well on simple tasks and fail on more complex tasks?</pre> <p>So it seems the model prefers the word over the colour as well! Similar toÂ us!?</p> <p>Several other questions might be interesting to investigate next:</p> <p>1- What makes the newer model better with thisÂ margin?</p> <p>2- Has the model seen these types of images or there is an emergence of image understanding?</p> <p>3- How are these models compared to human intelligence?</p> <p>4- Can we use these models as a human brain simulator and study theÂ brain?</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=bdb51a9f4f6f" width="1" height="1" alt=""></p> </body></html>