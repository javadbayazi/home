<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://javadbayazi.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://javadbayazi.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-12-15T01:52:10+00:00</updated><id>https://javadbayazi.github.io/feed.xml</id><title type="html">blank</title><subtitle>Mohammad-Javad Darvishi Bayazi (Javad Bayazi) website </subtitle><entry><title type="html">Time Series Forecasting Interactive Tool</title><link href="https://javadbayazi.github.io/blog/2024/ts/" rel="alternate" type="text/html" title="Time Series Forecasting Interactive Tool"/><published>2024-09-27T18:37:00+00:00</published><updated>2024-09-27T18:37:00+00:00</updated><id>https://javadbayazi.github.io/blog/2024/ts</id><content type="html" xml:base="https://javadbayazi.github.io/blog/2024/ts/"><![CDATA[<h1 id="interactive-tool-for-deep-learning-models-in-time-series-forecasting">Interactive Tool for Deep Learning Models in Time Series Forecasting</h1> <p>Time series forecasting is a technique used to predict future values based on previously observed values over time. If you have data collected over time, such as sales figures, temperatures, or stock prices, you can analyze historical patterns to make informed predictions about future trends. This can be incredibly useful in various fields, including finance, economics, and operations.</p> <h2 id="try-our-interactive-tool">Try Our Interactive Tool</h2> <p>We invite you to explore our interactive tool, where you can paste your own time series data and see the forecast generated by advanced deep learning models. Hereâ€™s how it works:</p> <ol> <li> <p><strong>Input Your Data</strong>: Simply copy and paste your time series data into the provided text box. The data should be in a simple format, separated by commas.</p> <p>For example:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>112, 118, 132, 129, 121, 135, 148, 148, 136, 119
</code></pre></div> </div> </li> <li> <p><strong>Adjust the Forecast Horizon</strong>: Use the slider to select the forecast horizon in months. This allows you to specify how far into the future you want the predictions to extend.</p> </li> <li> <p><strong>Automatic Forecasting</strong>: Once youâ€™ve pasted your data and adjusted the forecast horizon, the system will automatically generate forecasts for you, providing visualizations to help you understand the predictions.</p> </li> <li> <p><strong>Visualize the Results</strong>: The interactive tool will plot both your historical data and the forecast, complete with confidence intervals, so you can see the predicted values in context.</p> </li> </ol> <iframe src="https://javadbayazi-bfm.hf.space" frameborder="0" width="850" height="850"></iframe> <h2 id="conclusion">Conclusion</h2> <p>With this simple yet powerful interactive tool, you can easily see how deep learning models can be applied to time series forecasting. Itâ€™s a great way to explore the potential of advanced machine learning techniques in analyzing and predicting future trends from historical data.</p> <p>Feel free to experiment with your data and see how accurate the forecasts can be!</p> <p>For comments, feedback, or any questions, please reach out to me on <a href="https://www.linkedin.com/in/javadbayazi/">LinkedIn</a>.</p>]]></content><author><name></name></author><category term="interactive-tools"/><category term="forecasting,"/><category term="time-series,"/><category term="interactive-tool"/><summary type="html"><![CDATA[An interactive tool for time series forecasting]]></summary></entry><entry><title type="html">Psycho-LLaVA: Psychology of Large Language and Vision Assistant</title><link href="https://javadbayazi.github.io/blog/2024/psycho-llava-psychology-of-large-language-and-vision-assistant/" rel="alternate" type="text/html" title="Psycho-LLaVA: Psychology of Large Language and Vision Assistant"/><published>2024-04-08T22:11:22+00:00</published><updated>2024-04-08T22:11:22+00:00</updated><id>https://javadbayazi.github.io/blog/2024/psycho-llava-psychology-of-large-language-and-vision-assistant</id><content type="html" xml:base="https://javadbayazi.github.io/blog/2024/psycho-llava-psychology-of-large-language-and-vision-assistant/"><![CDATA[<p>Here we are curious to see how a vision language does when it comes to Stroop Color and Word Test. In humans, there is a very interesting effect called the Stroop effect (seeÂ <a href="https://en.wikipedia.org/wiki/Stroop_effect">here</a>).</p> <p>Please go <a href="https://faculty.washington.edu/chudler/java/ready.html">here</a> and do the experiment first and then come back, itâ€™sÂ fun!!</p> <p>Now let&#39;s see how does LLaVA the task. It will blow yourÂ mind!</p> <blockquote>ğŸŒ‹ <a href="https://llava-vl.github.io/">LLaVA: Large Language and Vision Assistant</a></blockquote> <h3>LLaVA (llava-v1.5â€“13b-4bit)</h3> <p>First, we ar using <a href="https://huggingface.co/spaces/badayvedat/LLaVA">this</a> demo on HF.<br/>Letâ€™s try a few examples:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/640/1*qfIqbvuSPe7S81zMPfrkdA.png"/><figcaption>What do you see? Did you say â€œBlueâ€ orÂ Redâ€?</figcaption></figure> <p>We give the model a picture of the word â€œBLUEâ€ with red ink and ask the model â€œIn this experiment, you are required to say the color of the word, not what the word says. one wordâ€. What would youÂ expect?</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*4K74RJKB5iek4HZcFCWj7w.png"/><figcaption>Wrong!!!</figcaption></figure> <p>It was pretty disappointing! But what about the newerÂ version?</p> <h3>LLaVA (llava-v1.6â€“34b)</h3> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*67LjdHDnlfDyIX81ZdrxKw.png"/><figcaption>Wow!!</figcaption></figure> <p>It did really well, and it seems that the newÂ version</p> <blockquote>â€œ<a href="https://llava-vl.github.io/blog/2024-01-30-llava-next/">LLaVA-NeXT: Improved reasoning, OCR, and world knowledge</a>â€</blockquote> <p>has improved!</p> <p>But can we still fool it? letâ€™sÂ try!</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*N1LlpkyjPqSCifghnPvmLg.png"/><figcaption>Wow! it is impressive!</figcaption></figure> <p>It seems the newest version of LLaVA really has an understanding ofÂ images!</p> <p>So to answer our question, does LLaVA have the StroopÂ effect?</p> <p>It seems that the LLaVA v1.6 is pretty robust and does not show any sign of theÂ effect!</p> <p>But wait! Do you think the model can do the taskÂ right?</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*-H8Etn6NkuaFN49v_9B1Gw.png"/><figcaption>The model focused on text!!! And wrote the words in the first row correctly but failed theÂ rest!</figcaption></figure> <p>So maybe the model has the StroopÂ effect?</p> <p>Another failureÂ example:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*HI7lo7pD4V5Ky8R-9t_omw.png"/><figcaption>Another failure example ofÂ LLaVA</figcaption></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*RzaWSk5SdhI-8p_kB8fMOg.png"/><figcaption>Another failure example ofÂ LLaVA</figcaption></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*UD6M4gFbCjwkr76-SixZZg.png"/><figcaption>Another failure example of LLaVA, the model keeps generating!!!</figcaption></figure> <p>What if we create a dataset and try to evaluate the model? A dataset of images with 5by5 grids of colourful words. We benchmarked `llava-hf/llava-v1.6-mistral-7b-hfâ€™ models with 200Â samples.</p> <p>for this prompt â€œ In this experiment you are required to say the color of the word, not what the word says. Read the list as fast as you can.â€ the scoresÂ are:</p> <pre>{&#39;rouge1&#39;: 0.2468631578947369, &#39;rouge2&#39;: 0.05287940379403794, &#39;rougeL&#39;: 0.171221052631579, &#39;rougeLsum&#39;: 0.17204210526315794}<br />{&#39;bleu&#39;: 0.08737648301423374, &#39;precisions&#39;: [0.19452054794520549, 0.13663911845730028, 0.08116343490304709, 0.02701949860724234], &#39;brevity_penalty&#39;: 1.0, &#39;length_ratio&#39;: 3.7244897959183674, &#39;translation_length&#39;: 3650, &#39;reference_length&#39;: 980}</pre> <p>for this prompt â€œ In this experiment you are required to say what the word says, not what the color of the word is. Read the list as fast as you can.â€ the scoresÂ are:</p> <pre>{&#39;rouge1&#39;: 0.40956600104156504, &#39;rouge2&#39;: 0.18901720719876702, &#39;rougeL&#39;: 0.3243491992969437, &#39;rougeLsum&#39;: 0.32076109917651274}<br />{&#39;bleu&#39;: 0.15035696220994574, &#39;precisions&#39;: [0.26395039858281666, 0.18453976764968721, 0.130297565374211, 0.0805277525022748], &#39;brevity_penalty&#39;: 1.0, &#39;length_ratio&#39;: 2.304081632653061, &#39;translation_length&#39;: 2258, &#39;reference_length&#39;: 980}So why does the model do well on simple tasks and fail on more complex tasks?</pre> <p>So it seems the model prefers the word over the colour as well! Similar toÂ us!?</p> <p>Several other questions might be interesting to investigate next:</p> <p>1- What makes the newer model better with thisÂ margin?</p> <p>2- Has the model seen these types of images or there is an emergence of image understanding?</p> <p>3- How are these models compared to human intelligence?</p> <p>4- Can we use these models as a human brain simulator and study theÂ brain?</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=bdb51a9f4f6f" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry></feed>